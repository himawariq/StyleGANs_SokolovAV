{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e9ba8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from functools import partialmethod\n",
    "import gradio as gr\n",
    "from gradio.mix import Series\n",
    "from transformers import pipeline, FSMTForConditionalGeneration, FSMTTokenizer\n",
    "from rudalle.pipelines import generate_images\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd9a7040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "◼️ Malevich is 1.3 billion params model from the family GPT3-like, that uses Russian language and text+image multi-modality.\n",
      "tokenizer --> ready\n",
      "Working with z of shape (1, 256, 32, 32) = 262144 dimensions.\n",
      "vae --> ready\n"
     ]
    }
   ],
   "source": [
    "# disable tqdm logging from the rudalle pipeline\n",
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "translation_model = FSMTForConditionalGeneration.from_pretrained(\"facebook/wmt19-en-ru\", torch_dtype=torch.float16).to(\n",
    "    device)\n",
    "translation_tokenizer = FSMTTokenizer.from_pretrained(\"facebook/wmt19-en-ru\")\n",
    "dalle = get_rudalle_model(\"Malevich\", pretrained=True, fp16=True, device=device)\n",
    "tokenizer = get_tokenizer()\n",
    "vae = get_vae().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(text):  \n",
    "    text = ''\n",
    "    pil_images = []\n",
    "    scores = []\n",
    "    \n",
    "    seed_everything(6955)\n",
    "\n",
    "    for top_k, top_p, images_num in [\n",
    "    (1024, 0.98, 3),\n",
    "    (512, 0.97, 3),\n",
    "    (384, 0.96, 3), \n",
    "]:\n",
    "        _pil_images, _scores = generate_images(text, tokenizer, dalle, vae, top_k=top_k, images_num=images_num, top_p=top_p)\n",
    "        pil_images += _pil_images\n",
    "        scores += _scores\n",
    "    top_images, clip_scores = cherry_pick_by_clip(pil_images, text, ruclip, ruclip_processor, device=device, count=6)\n",
    "    show(top_images, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2317faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_wrapper(text: str):\n",
    "    input_ids = translation_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    outputs = translation_model.generate(input_ids.to(device))\n",
    "    decoded = translation_tokenizer.decode(outputs[0].float(), skip_special_tokens=True)\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def dalle_wrapper(prompt: str):\n",
    "    \n",
    "    pil_images = []\n",
    "    \n",
    "    top_k, top_p = random.choice([\n",
    "        (1024, 0.98),\n",
    "        (512, 0.97)\n",
    "    ])\n",
    "\n",
    "    _images, _ = generate_images(\n",
    "        prompt,\n",
    "        tokenizer,\n",
    "        dalle,\n",
    "        vae,\n",
    "        top_k=top_k,\n",
    "        images_num=1,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    pil_images += _images\n",
    "    \n",
    "    \n",
    "    \n",
    "    return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc171d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1490920be3f249f69a57e2b0ed4adaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=RGB size=256x256 at 0x157FCE0E9D0>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dalle_wrapper('Пудж на миде')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a84410",
   "metadata": {},
   "outputs": [],
   "source": [
    ",\n",
    "        (384, 0.96),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97e05aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.title('Text generation')\n",
    "    menu = ['Сгенерировать контент','О проекте']\n",
    "    choice = st.sidebar.selectbox('Menu',menu)\n",
    "    if choice == 'Сгенерировать контент':\n",
    "        st.subheader('Сгенерировать контент')\n",
    "        with st.form(key='123'):\n",
    "            raw_text = st.text_area('Введите тут')\n",
    "            submit_text = st.form_submit_button(label='Сгенерировать!')\n",
    "\n",
    "    \n",
    "\n",
    "    else:\n",
    "        st.subheader('О проекте')\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a0a2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
